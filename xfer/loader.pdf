"""
Loader for Compensation Plans
Loads PDFs, extracts text, and saves as JSON for further processing.
"""

import os
import json
import logging
from pathlib import Path
from typing import List, Dict, Optional, Union, Any
import fitz  # PyMuPDF for PDF text extraction
from config.config import config  # Import your centralized configuration

logger = logging.getLogger("loader")

def extract_text_from_pdf(pdf_path: Union[str, Path]) -> str:
    """
    Extracts text from a PDF file using PyMuPDF.
    
    Args:
        pdf_path: Path to the PDF file
        
    Returns:
        Extracted text as a string
        
    Raises:
        FileNotFoundError: If the PDF file doesn't exist.
    """
    pdf_path = Path(pdf_path)
    
    if not pdf_path.exists():
        logger.error(f"❌ PDF file not found: {pdf_path}")
        raise FileNotFoundError(f"PDF not found: {pdf_path}")
        
    if not pdf_path.is_file():
        logger.error(f"❌ Path is not a file: {pdf_path}")
        raise ValueError(f"Not a file: {pdf_path}")
    
    try:
        with fitz.open(str(pdf_path)) as doc:
            page_count = len(doc)
            text_parts = []
            for i, page in enumerate(doc):
                page_text = page.get_text("text")
                if page_text.strip():
                    text_parts.append(f"--- Page {i+1} of {page_count} ---\n{page_text}")
            text = "\n".join(text_parts)
            logger.info(f"✅ Extracted {len(text)} characters from {page_count} pages in {pdf_path}")
            return text
    except Exception as e:
        logger.error(f"❌ Error extracting text from {pdf_path}: {str(e)}")
        raise RuntimeError(f"Failed to extract text from PDF: {str(e)}") from e

def save_as_json(text: str, source_filename: str, output_dir: Union[str, Path] = None) -> Path:
    """
    Saves extracted text as JSON with metadata into the specified output directory.
    
    Args:
        text: Extracted text content.
        source_filename: Original filename (used to generate output filename).
        output_dir: Optional output directory. If not provided, defaults to config.PLAN_OUTPUT_DIR/raw_json.
        
    Returns:
        Path to the created JSON file.
    """
    # Use the output directory from config appended with 'raw_json' if not provided
    if output_dir is None:
        output_dir = Path(config.PLAN_OUTPUT_DIR) / "raw_json"
    else:
        output_dir = Path(output_dir)
    
    # Ensure the output directory exists
    try:
        output_dir.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        logger.error(f"❌ Failed to create output directory {output_dir}: {str(e)}")
        raise
    
    # Clean up source filename and create output path
    source_filename = os.path.basename(source_filename)
    base_name = source_filename.replace(".pdf", "")
    json_path = output_dir / f"{base_name}.json"
    
    # Prepare metadata
    data = {
        "title": base_name,
        "source_file": source_filename,
        "content": text,
        "character_count": len(text),
        "timestamp": __import__('datetime').datetime.now().isoformat()
    }
    
    try:
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        logger.info(f"✅ Saved raw JSON ({len(text)} chars): {json_path}")
        return json_path
    except Exception as e:
        logger.error(f"❌ Failed to save JSON to {json_path}: {str(e)}")
        raise

def load_pdfs(directory: Union[str, Path]) -> List[Path]:
    """
    Loads all PDFs from a directory, extracts text, and saves as JSON.
    
    Args:
        directory: Directory containing PDF files.
        
    Returns:
        List of paths to the created JSON files.
        
    Raises:
        FileNotFoundError: If the directory doesn't exist.
    """
    directory = Path(directory)
    
    if not directory.exists():
        logger.error(f"❌ Directory not found: {directory}")
        raise FileNotFoundError(f"Directory not found: {directory}")
    
    if not directory.is_dir():
        logger.error(f"❌ Path is not a directory: {directory}")
        raise NotADirectoryError(f"Not a directory: {directory}")
    
    pdf_files = list(directory.glob("*.pdf"))
    if not pdf_files:
        logger.warning(f"⚠ No PDFs found in {directory}")
        return []
    
    logger.info(f"📂 Found {len(pdf_files)} PDF files in {directory}")
    
    json_files = []
    for pdf_path in pdf_files:
        try:
            text = extract_text_from_pdf(pdf_path)
            if not text.strip():
                logger.warning(f"⚠ Extracted empty text from {pdf_path}, skipping")
                continue
            json_path = save_as_json(text, pdf_path.name)
            json_files.append(json_path)
        except Exception as e:
            logger.error(f"❌ Failed to process {pdf_path}: {str(e)}")
            continue
    
    logger.info(f"✅ Processed {len(json_files)} of {len(pdf_files)} PDFs successfully")
    return json_files

def load_text_files(directory: Union[str, Path]) -> List[Path]:
    """
    Loads all text files (txt) from a directory and saves as JSON.
    
    Args:
        directory: Directory containing text files.
        
    Returns:
        List of paths to the created JSON files.
    """
    directory = Path(directory)
    
    if not directory.exists():
        logger.error(f"❌ Directory not found: {directory}")
        raise FileNotFoundError(f"Directory not found: {directory}")
    
    text_files = list(directory.glob("*.txt"))
    if not text_files:
        logger.warning(f"⚠ No text files found in {directory}")
        return []
    
    logger.info(f"📂 Found {len(text_files)} text files in {directory}")
    
    json_files = []
    for text_path in text_files:
        try:
            with open(text_path, 'r', encoding='utf-8', errors='replace') as f:
                text = f.read()
            if not text.strip():
                logger.warning(f"⚠ Empty text file: {text_path}, skipping")
                continue
            json_path = save_as_json(text, text_path.name)
            json_files.append(json_path)
        except Exception as e:
            logger.error(f"❌ Failed to process {text_path}: {str(e)}")
            continue
    
    logger.info(f"✅ Processed {len(json_files)} of {len(text_files)} text files successfully")
    return json_files

def load_js_files(directory: Union[str, Path]) -> List[Path]:
    """
    Loads all JavaScript files (js) from a directory and saves as JSON.
    
    Args:
        directory: Directory containing JS files.
        
    Returns:
        List of paths to the created JSON files.
    """
    directory = Path(directory)
    
    js_files = list(directory.glob("*.js"))
    if not js_files:
        logger.warning(f"⚠ No JS files found in {directory}")
        return []
    
    logger.info(f"📂 Found {len(js_files)} JS files in {directory}")
    
    json_files = []
    for js_path in js_files:
        try:
            with open(js_path, 'r', encoding='utf-8', errors='replace') as f:
                text = f.read()
            json_path = save_as_json(text, js_path.name)
            json_files.append(json_path)
        except Exception as e:
            logger.error(f"❌ Failed to process {js_path}: {str(e)}")
            continue
    
    return json_files

def load_all_documents(directory: Union[str, Path]) -> List[Path]:
    """
    Loads all supported document types from a directory.
    
    Args:
        directory: Directory containing files.
        
    Returns:
        List of paths to the created JSON files.
    """
    directory = Path(directory)
    
    pdf_results = load_pdfs(directory)
    text_results = load_text_files(directory)
    js_results = load_js_files(directory)
    
    all_results = pdf_results + text_results + js_results
    
    if not all_results:
        logger.warning(f"⚠ No supported files found in {directory}")
    else:
        logger.info(f"✅ Processed {len(all_results)} total files from {directory}")
    
    return all_results